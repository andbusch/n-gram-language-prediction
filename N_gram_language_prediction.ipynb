{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxpnuETSDQLGRHkgbBdrRy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andbusch/n-gram-language-prediction/blob/main/N_gram_language_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iPwJ9C0Sq1cq"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ghGrMxU03QxEBcqt292bcC6Ej-7Oso0h' -O czech.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lQzOJ0QzLkw",
        "outputId": "9d103c8c-0dd6-4df0-fdff-3f0c8c835c9b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rczech.txt             0%[                    ]       0  --.-KB/s               \rczech.txt           100%[===================>] 354.30K  --.-KB/s    in 0.03s   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=1HYbaf-zECkGPJIq2su13AkLHBOapgLHh' -O english.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lClg4WhzbbQ",
        "outputId": "bbbccf9a-9573-4bb4-a613-b67acefd3623"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\renglish.txt           0%[                    ]       0  --.-KB/s               \renglish.txt         100%[===================>] 154.14K  --.-KB/s    in 0.02s   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=18mBhXOWPNX1WtTBFnZyFG_ayGFJIlsaS' -O japanese.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDRXsp0Szee5",
        "outputId": "6108d6a5-fe90-442e-f9d8-865d4a2ce890"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rjapanese.txt          0%[                    ]       0  --.-KB/s               \rjapanese.txt        100%[===================>] 478.96K  --.-KB/s    in 0.03s   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=1WQyJlNtqEmMTwgLKcOVKuIRCha5JVLAk' -O german.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAk927H-zhGy",
        "outputId": "bd3f7558-eb87-4934-f0af-6381c3a4326b"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rgerman.txt            0%[                    ]       0  --.-KB/s               \rgerman.txt          100%[===================>] 948.41K  --.-KB/s    in 0.04s   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "S4rxG4xdjs-K"
      },
      "outputs": [],
      "source": [
        "class N_Gram():\n",
        "  def __init__(self, n, lang):\n",
        "    self.bigram_dict = collections.defaultdict(lambda: 0)\n",
        "    self.__n = n\n",
        "    self.lang = lang\n",
        "    self.bigram_counter = 0\n",
        "\n",
        "  def train_data(self, filename):\n",
        "    print(\"Training \" + self.lang + \" Model\")\n",
        "    with open(filename) as f0:\n",
        "      self.word_list = f0.readlines()\n",
        "      for wd in self.word_list[0:int(0.60 * len(self.word_list))]:\n",
        "        self.process_word(wd)\n",
        "\n",
        "  def process_word(self, wd): # add word to list of n-grams\n",
        "    wd = '#'+wd+'#'\n",
        "    for i in range(len(wd) - self.__n + 1):\n",
        "      self.bigram_dict[wd[i:i+self.__n]] += 1\n",
        "      self.bigram_counter += 1\n",
        "\n",
        "\n",
        "  def compute_probability(self, wd) -> float: # find the probability given a word and a language, will be used on testing data\n",
        "    wd = '#'+wd+'#'\n",
        "    prob = 0.0\n",
        "    for i in range(len(wd) - self.__n + 1):\n",
        "      if self.bigram_dict[wd[i:i+self.__n]] == 0:\n",
        "        #prob += -10000\n",
        "        prob += np.log(1 / self.bigram_counter)\n",
        "      else:\n",
        "        prob += np.log(self.bigram_dict[wd[i:i+self.__n]] / self.bigram_counter)\n",
        "    return prob\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_size = 2\n",
        "\n",
        "def test_data(langs, i):\n",
        "    num_correct = 0\n",
        "    total_counted = 0\n",
        "    with open(f'predictions_{langs[i].lang}_{bigram_size}.txt', 'w') as file:\n",
        "      for word in langs[i].word_list[int(0.60 * len(langs[i].word_list)):]:  # iterate through remaining 40% of word list\n",
        "        predicted = [-1000.0, \"language\"]  # Re-initialize for each word\n",
        "        for lang in langs:\n",
        "          tot_prob = total_prob(word, lang, langs)\n",
        "          if tot_prob > predicted[0]:\n",
        "            predicted = [tot_prob, lang.lang]\n",
        "          file.write(f\"Word: {word} predicted to language {predicted[1]} with probability {predicted[0]}\\n\")\n",
        "          if predicted[1] == langs[i].lang:\n",
        "            num_correct+=1\n",
        "          total_counted+=1\n",
        "      file.write(f\"Total Correct: {num_correct} out of {total_counted} success % = {num_correct / total_counted}\")\n",
        "\n",
        "def total_prob(word, lang, langs) -> float:\n",
        "    return lang.compute_probability(word) * (1 / 4) / prob_word(word, langs)\n",
        "\n",
        "def prob_word(word, langs) -> float:\n",
        "    num_bigrams = 0\n",
        "    num_occ = 0\n",
        "\n",
        "    for lang in langs:\n",
        "        num_bigrams += lang.bigram_counter\n",
        "\n",
        "    if num_bigrams == 0:\n",
        "        return 0\n",
        "\n",
        "    word = '#' + word + '#'\n",
        "    for i in range(len(word) - bigram_size + 1):\n",
        "        for lang in langs:\n",
        "            num_occ += lang.bigram_dict.get(word[i:i+bigram_size], 0)  # Use .get to handle missing bigrams\n",
        "\n",
        "    return num_occ / num_bigrams if num_bigrams > 0 else 0\n"
      ],
      "metadata": {
        "id": "MW4unjC9gCic"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english = N_Gram(bigram_size, \"english\")\n",
        "english.train_data(\"english.txt\")\n",
        "\n",
        "czech = N_Gram(bigram_size, \"czech\")\n",
        "czech.train_data(\"czech.txt\")\n",
        "\n",
        "german = N_Gram(bigram_size, \"german\")\n",
        "german.train_data(\"german.txt\")\n",
        "\n",
        "japanese = N_Gram(bigram_size, \"japanese\")\n",
        "japanese.train_data(\"japanese.txt\")\n",
        "\n",
        "langs = [english, czech, german, japanese]\n",
        "\n",
        "for i in range(0,4):\n",
        "  test_data(langs, i)\n",
        "\n",
        "# test_data(langs, \"english.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8PEX0lhoaLM",
        "outputId": "e6b94a34-9090-4f52-ada1-6b7f07353bbc"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training english Model\n",
            "Training czech Model\n",
            "Training german Model\n",
            "Training japanese Model\n"
          ]
        }
      ]
    }
  ]
}